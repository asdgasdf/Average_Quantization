# Benchmark Memory Usage and Training Speed on Torchvision Models

Benchmark Memory Usage and Training Speed modified from [actnn/mem_speed_benchmark](https://github.com/ucbrise/actnn/tree/main/mem_speed_benchmark).

## Prepare dataset
Put the ImageNet dataset to `~/imagenet`

## Benchmark Memory Usage
```bash
DEBUG_MEM=True python3 train.py ~/imagenet --arch ARCH -b BATCH_SIZE --alg ALGORITHM --bit BIT --aq-bit AQ-BIT --gpu GPU-NUM
```
The choices for ARCH are {```resnet50```, ```resnet152```, ```wide_resnet101_2```, ```densenet201```}. In the paper, we experiment ARCH with ```resnet50```. 

The choices for ALGORITHM are {```exact```, ```actnn-L0```, ```actnn-L1```, ```actnn-L2```, ```actnn-L3```, ```actnn-L4```, ```actnn-L5```}. In the paper, we experiment ARCH with ```actnn-L3```.

In the paper, we experiment BIT (target average preicion) with ```2```, ```1.5```, ```1.25```, or ```1```.

In the paper, we experiment AQ-BIT with ```1``` and ```0.5```, which generated by group average of 2 and 4 elements, respectively, and 2-bit quantization. 

For example, the memory required to ActNN_AQ-0.5bit when target average precision is 1.25-bit can be acheived by
```bash
DEBUG_MEM=True python3 train.py ~/imagenet --arch resnet50 -b 256 --alg actnn-L3 --bit 1.25 --aq-bit 0.5 --gpu 0
```

## Benchmark Training Speed
```bash
DEBUG_SPEED=True python3 train.py ~/imagenet --arch ARCH -b BATCH_SIZE --alg ALGORITHM --bit BIT --aq-bit AQ-BIT --gpu GPU-NUM
```
The choices for ARCH are {```resnet50```, ```resnet152```, ```wide_resnet101_2```, ```densenet201```}. In the paper, we experiment ARCH with ```resnet50```. 

The choices for ALGORITHM are {```exact```, ```actnn-L0```, ```actnn-L1```, ```actnn-L2```, ```actnn-L3```, ```actnn-L4```, ```actnn-L5```}. In the paper, we experiment ARCH with ```actnn-L3```.

In the paper, we experiment BIT (target average preicion) with ```2```, ```1.5```, ```1.25```, or ```1```.

In the paper, we experiment AQ-BIT with ```1``` and ```0.5```, which generated by group average of 2 and 4 elements, respectively, and 2-bit quantization. 

For example, the training speed by ActNN_AQ-0.5bit when target average precision is 1.25-bit can be acheived by
```bash
DEBUG_SPEED=True python3 train.py ~/imagenet --arch resnet50 -b 128 --alg actnn-L3 --bit 1.25 --aq-bit 0.5 --gpu 0
```

## Find maximum batch sizes and model sizes
```bash
python exp_mem_speed.py --binary_search_max_batch
python exp_mem_speed.py --binary_search_max_input_size
python exp_mem_speed.py --binary_search_max_layer
python exp_mem_speed.py --binary_search_max_width
```
